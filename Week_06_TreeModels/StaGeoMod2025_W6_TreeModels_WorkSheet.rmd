---
title: "Week 6 Practical – Tree Based Models"
subtitle: "Using CART, Random Forests, and Boosted Regression trees"
author: "Student Name: Laura Ryge Koch"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    number_sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
```

## Setup

**Packages used**: Base R for processing; `rpart`, `randomForest`, `gbm` for modelling; `ggplot2` for plots.

```{r setup2, message=FALSE, warning=FALSE}
# Load required packages
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(ggplot2)
theme_set(theme_bw(base_size = 12))

# Set seed for reproducible results
set.seed(123)


```

## About the Data

The objective is to predict deciduous broadleaf forests (TBMF) from climate variables, then map predictions globally. The dataset includes three files:

- `BiomeData.csv` – Long-format table of sampled sites (≈500 points per vegetation type) with a unique ID and a biome label.
- `ClimateData.csv` – Site-level climate variables with matching IDs.
- `WorldClimateData.csv` – Global grid of climate variables for prediction (includes longitude, latitude, and the same predictor column names).

---

## Task 1 – Building a Presence–Absence Matrix for TBMF (10–12 min)

**Objective:**

The first task involves converting the long-format `BiomeData.csv` (containing site `ID` and `Biome` columns) into a **wide presence–absence** format for the focal biome (**Temperate Broadleaf & Mixed Forests**, abbreviated as TBMF). The goal is to create a binary response variable (1 = present, 0 = absent) for each unique location `ID`.


```{r}
# ============================================================================
# STEP 1: LOAD DATA FILES
# ============================================================================

# Read biome dataset
# HINT: stringsAsFactors keeps text as characters, [,-1] removes first column
biome <- read.csv("BiomeData.csv", 
                  stringsAsFactors = TRUE)[,-1]  # Remove column 1

# Read climate dataset
clim  <- read.csv("ClimateData.csv", 
                  stringsAsFactors = TRUE)[,-1]

# ============================================================================
# STEP 2: CREATE PRESENCE-ABSENCE DATA FOR TBMF
# ============================================================================

# Extract all unique location IDs
unique_ids <- unique(biome$ID)  # Get unique values from biome$ID

# Create presence-absence indicator for TBMF
# HINT: tapply() groups data and applies a function to each group
# HINT: any() returns TRUE if at least one value is TRUE
DBF_flag <- tapply(
  # Check if biome name matches TBMF (creates TRUE/FALSE)
  biome$BIOME.Name == "Tropical & Subtropical Moist Broadleaf Forests",  # Complete the biome name
  
  # Group by location ID
  biome$ID,  # Which column contains the ID?
  
  # Function: Check if ANY row has TBMF, convert to integer (1/0)
  function(x) as.integer(any(x))  # Use any() function
)

# Convert results to a data frame
dbf_df <- data.frame(
  ID = as.integer(names(DBF_flag)),    # Extract IDs from names
  TBMF = as.integer(DBF_flag),         # Presence/absence values
  row.names = NULL
)

# ============================================================================
# STEP 3: VERIFY THE RESULTS
# ============================================================================

# Display frequency table
# HINT: table() counts occurrences of each value
table(dbf_df)  # Count TBMF values (0 and 1)
```

**Reflection Questions:**

1. If some sites contain multiple biomes, why is `any()` a reasonable function for aggregating to presence–absence? 
*any() checks if the biome occurs at least once at a site, summarizing multiple records into a single presence (1) or absence (0)*

2. What challenges arise if the positive class ("Temperate Broadleaf & Mixed Forests") is very rare or very common in the dataset?
*Rare positive class--> class imbalance, poor model learning. very common positive class -->low variability, poor discrimination.*

3. Why is a site-level binary response preferable to a multi-class target for this exercise?
*Binary response simplifies the problem and focuses on one biome, avoids multi-class imbalance, is easier to link to climate data*

---

## Task 2 – Matching Climate Data to Sites (8–10 min)

**Objective:**

This task merges the site-level TBMF response variable with corresponding climate predictors using base R's `merge()` function. Rows with missing climate values are removed to ensure complete datasets for modeling.

```{r}
# ============================================================================
# STEP 1: MERGE BIOME AND CLIMATE DATA
# ============================================================================

# Combine TBMF presence/absence with climate measurements
# HINT: merge() combines data frames by a common column
dat <- merge(
  dbf_df,              # TBMF data frame (dbf_df)
  clim,              # Climate data frame (clim)
  by = "ID",       # Column name to match on (ID)
  all.x = TRUE       # Keep all rows from first data frame? (TRUE/FALSE)
)

# ============================================================================
# STEP 2: REMOVE ROWS WITH MISSING CLIMATE DATA
# ============================================================================

# Identify climate predictor columns
# HINT: setdiff() finds elements in first set but not in second
pred_cols <- setdiff(
  colnames(dat),           # All column names from dat
  c("ID", "TBMF")          # Columns to exclude (ID and TBMF)
)

# Keep only rows with complete climate data
# HINT: complete.cases() returns TRUE for rows with no missing values
dat <- dat[
  complete.cases(dat[, pred_cols, drop = FALSE]),  # Check predictor columns
]

# Display dataset structure
str(dat)  # Show structure of dat

# ============================================================================
# STEP 3: CHECK CLASS BALANCE
# ============================================================================

# Calculate proportion of absent (0) vs present (1)
# HINT: prop.table() converts counts to proportions
prop.table(table(dat$TBMF))  # Create table of TBMF column
```

**Reflection Questions:**

1. When removing locations with incomplete climate data, what types of locations might be systematically excluded, and how could this bias the understanding of TBMF distribution?
*such locations might be remote, high-altitude, or poorly surveyed. removing them could therefore bias results toward well-sampled, accessible areas => could misrepresent the true TBMF distribution*

2. Why is class balance important for evaluating model accuracy?
*important because imbalanced data can make accuracy misleading. a model can appear accurate by predicting the majority class but then missing the minority class*

3. Which climate variables might be collinear, and why do tree-based models typically handle collinearity well?
*e.g. mean annual temperature and temperature seasonality, precipitation and precipitation seasonality - because they are derived from overlapping climate processes. tree-based models handle collinearity well because they split data on one variable at a time and are insensitive to correlations between predictors*

---

## Task 3a – CART: Fitting and Pruning (10 min)

**Objective:**

A decision tree model is built to predict TBMF presence/absence based on climate variables. Cross-validation is used to avoid overfitting, the tree is pruned to optimal size, decision rules are visualized, and the most important climate variables are identified.

```{r}
# ============================================================================
# STEP 1: BUILD THE INITIAL CLASSIFICATION TREE
# ============================================================================

# Create model formula: TBMF ~ all climate variables
# HINT: paste() with collapse joins elements with a separator
form <- as.formula(
  paste(
    "TBMF ~",                            # Response variable name
    paste(pred_cols, collapse = " + ")      # Join predictors with " + "
  )
)
# Fit a Classification And Regression Tree
# HINT: method = "class" for classification, cp = complexity parameter
cart0 <- rpart(
  form,                      # Formula (form)
  data = dat,               # Dataset (dat)
  method = "class",           # "class" for classification
  xval = 10,               # Number of cross-validation folds (10)
  control = rpart.control(
    cp = 0.001                # Small value like 0.001 for large initial tree
  )
)

# ============================================================================
# STEP 2: FIND THE OPTIMAL TREE COMPLEXITY
# ============================================================================

# Display cross-validation results
cp_tbl <- printcp(cart0)  # Print CP table for cart0

# Find tree size with minimum CV error
# HINT: which.min() returns position of minimum value
best_row <- which.min(cp_tbl[, "xerror"])    # Find min "xerror"
cp_min   <- cp_tbl[best_row, "CP"]            # Get CP at best row
xerr_min <- cp_tbl[best_row, "xerror"]            # Get xerror at best row
xerr_se  <- cp_tbl[best_row, "xstd"]            # Get xstd at best row

# Apply 1-standard-error rule
# HINT: max() finds the largest value meeting the condition
cp_1se <- max(
  cp_tbl[
    cp_tbl[, "xerror"] <= (xerr_min + xerr_se),  # xerr_min + xerr_se
    "CP"
  ]
)

# ============================================================================
# STEP 3: PRUNE THE TREE TO OPTIMAL SIZE
# ============================================================================

# Remove unnecessary branches
# HINT: prune() simplifies tree using CP threshold
cart <- prune(
  cart0,          # Original tree (cart0)
  cp = cp_1se      # Optimal CP (cp_1se)
)

# ============================================================================
# STEP 4: VISUALIZE THE DECISION TREE
# ============================================================================

# Create graphical representation
# HINT: type, extra, under, fallen.leaves control appearance
rpart.plot(
  cart,                   # Pruned tree (cart)
  type = 2,            # 2 shows split variables
  extra = 104,           # 104 shows probabilities and percentages
  under = TRUE,           # TRUE places info under boxes
  fallen.leaves = TRUE    # TRUE aligns terminal nodes at bottom
)

# ============================================================================
# STEP 5: IDENTIFY MOST IMPORTANT CLIMATE PREDICTORS
# ============================================================================

# Extract variable importance scores
cart_vi <- data.frame(
  Variable = names(cart$variable.importance),      # From cart
  Importance = as.numeric(cart$variable.importance) # From cart
)

# Sort from most to least important
# HINT: order() with negative value sorts descending
cart_vi <- cart_vi[order(-cart_vi$Importance), ]  # Sort by -Importance

# Display top predictors
head(cart_vi, 10)  # Show first 6 or 10 rows
```

**Reflection Questions:**

1. Why does pruning usually improve out-of-sample performance even if training accuracy drops?
*reduces tree complexity, removing splits that fit noise in the training data. This lowers overfitting, improving performance on unseen data (even if training accuracy drops)*

2. What does the `xerror` in the CP table represent, and how does it differ from external 10-fold CV?
*=the cross-validated relative error for a given tree size. It estimates generalization error, but is not independent external validation like 10‑fold CV on completely separate data*

3. How can the 1-SE choice be justified when describing results?
*the 1‑SE rule chooses the simplest tree whose error is within one standard error of the minimum. This balances accuracy and model simplicity, avoids overfitting and retains good predictive performance*

---

## Task 3b – CART: External 10-Fold Cross-Validation (10 min)

**Objective:**

External 10-fold cross-validation provides an honest assessment of how well the CART model predicts TBMF on completely new data.

**Performance metrics definitions:**

- **Accuracy**: Percentage of correct predictions (both present and absent)
- **Sensitivity**: Of locations truly having TBMF, percentage correctly identified
- **Specificity**: Of locations truly lacking TBMF, percentage correctly identified
- **Balanced Accuracy**: Average of sensitivity and specificity, giving equal weight to both classes


```{r}
# External 10-fold cross-validation
# HINT: make_folds() creates fold indices, k = number of folds
# Helper: 10-fold CV splitter
make_folds <- function(n, k = 10) {
  folds <- sample(rep(1:k, length.out = n))
  split(seq_len(n), folds)
}

folds <- make_folds(nrow(dat), k = 10)  # Number of rows in dat, 10 folds

# HINT: lapply() applies function to each fold
cart_cv <- do.call(rbind, lapply(folds, function(idx_test){
  # Split into training and test sets
  # HINT: -idx_test means "all rows except test indices"
  tr <- dat[-idx_test, ]  # Training: all except test indices
  te <- dat[idx_test, ]          # Test: only test indices
  
  # Train model on training set
  m  <- rpart(form, data = tr, method = "class", xval = 10, 
              control = rpart.control(cp = cp_1se))
  
  # Predict on test set
  # HINT: predict() with type="prob" gives probabilities
  p  <- predict(m, newdata = te, type = "prob")[, "1"]
  
  # Store results
  data.frame(ID = te$ID, TBMF = te$TBMF, prob = p)
}))

# Helper: accuracy metrics from a confusion table (binary, positive = 1)
# INPUTS:
#   truth - actual values (0 or 1) from the dataset
#   prob  - predicted probabilities (0 to 1) from the model
#   thr   - threshold for converting probabilities to predictions (default 0.5)
# OUTPUTS:
#   data frame with 4 metrics: Accuracy, Sensitivity, Specificity, BalancedAccuracy
acc_metrics <- function(truth, prob, thr = 0.5) {
  # Convert probabilities to binary predictions using threshold
  # If probability >= threshold, predict 1 (present), otherwise predict 0 (absent)
  pred <- ifelse(prob >= thr, 1, 0)
  # ============================================================================
  # CALCULATE CONFUSION MATRIX COMPONENTS
  # ============================================================================
  # True Positives (TP): Correctly predicted as present
  # Both truth and prediction are 1
  TP <- sum(truth == 1 & pred == 1)
  # True Negatives (TN): Correctly predicted as absent
  # Both truth and prediction are 0
  TN <- sum(truth == 0 & pred == 0)
  # False Positives (FP): Incorrectly predicted as present
  # Truth is 0 but prediction is 1 (Type I error)
  FP <- sum(truth == 0 & pred == 1)
  # False Negatives (FN): Incorrectly predicted as absent
  # Truth is 1 but prediction is 0 (Type II error)
  FN <- sum(truth == 1 & pred == 0)
  # ============================================================================
  # CALCULATE PERFORMANCE METRICS
  # ============================================================================
  # ACCURACY: Proportion of all predictions that were correct
  # Formula: (TP + TN) / Total
  # Interpretation: Overall correctness across both classes
  acc <- (TP + TN) / (TP + TN + FP + FN)
  
  # SENSITIVITY (Recall, True Positive Rate):
  # Of all actual positives, what proportion did we correctly identify?
  # Formula: TP / (TP + FN)
  # Interpretation: How good at finding what's really there
  # ifelse() prevents division by zero if no actual positives exist
  sens <- ifelse((TP + FN) == 0, NA, TP / (TP + FN))
  # SPECIFICITY (True Negative Rate):
  # Of all actual negatives, what proportion did we correctly identify?
  # Formula: TN / (TN + FP)
  # Interpretation: How good at confirming what's really absent
  # ifelse() prevents division by zero if no actual negatives exist
  spec <- ifelse((TN + FP) == 0, NA, TN / (TN + FP))
  # BALANCED ACCURACY:
  # Average of sensitivity and specificity
  # Gives equal weight to both classes regardless of class imbalance
  # na.rm = TRUE handles cases where sensitivity or specificity is NA
  bal_acc <- mean(c(sens, spec), na.rm = TRUE)
  # ============================================================================
  # RETURN RESULTS
  # ============================================================================
  # Return all metrics as a single-row data frame
  data.frame(
    Accuracy = acc, 
    Sensitivity = sens, 
    Specificity = spec, 
    BalancedAccuracy = bal_acc
  )
}

# Calculate performance metrics
# HINT: acc_metrics() needs true values, predictions, threshold (set to 0.5)
cart_cv_metrics <- acc_metrics(cart_cv$TBMF, cart_cv$prob, thr = 0.5)
cart_cv_metrics
```

**Reflection Questions:**

1. Why is nested cross-validation necessary (both internal `xval=10` within `rpart` AND external CV)? What would go wrong with only one level?
*Internal CV tunes the model. external CV fairly evaluates it. One level will be biased, overoptimistic performance.*

2. How should the 0.5 probability threshold used in the `acc_metrics` function be chosen depending on whether the goal is conservation planning, ecological research, or climate change prediction?
*in conservation, a lower threshold would be good for higher sensitivity. In ecological research, a balanced sensitivity and specificity. Climate change prediction => higher threshold for higher specificity*

3. Does random folding violate independence assumptions when nearby locations have similar climates and biomes (spatial autocorrelation)?
*Violates independence if spatial autocorrelation exists => should use spatial cross-validation instead*

---

## Task 4 – Random Forest (18–20 min)

**Objective:**

A Random Forest model is built—an ensemble of many decision trees that "vote" on predictions. The key parameter (`mtry`) is optimized, performance is evaluated through cross-validation, and the most important climate predictors are identified.

```{r}
# ============================================================================
# STEP 1: OPTIMIZE THE mtry PARAMETER
# ============================================================================

# Count climate predictor variables
p <- length(pred_cols)  # Number of predictor columns

# Create grid of mtry values to test
# HINT: seq() creates sequence, pmax() ensures minimum of 1
mtry_grid <- round(seq(1, p, length.out = 7)) # From 1 to p, create 7 values minimum

# Test each mtry value
# HINT: sapply() applies function to each element
oob <- sapply(mtry_grid, function(m) {  # Loop through mtry_grid
  # Build Random Forest
  # HINT: ntree = number of trees, mtry = variables per split
  rf_tmp <- randomForest(
    x = dat[, pred_cols],            # Predictor columns
    y = as.factor(dat$TBMF),    # Response as factor to make 
    ntree = 500,               # Number of trees (500)
    mtry = m,                # Current mtry value (m)
    importance = TRUE           # Calculate importance? (TRUE)
  )
  # Extract OOB error
  # HINT: Access error rate matrix, last row, "OOB" column
  rf_tmp$err.rate[rf_tmp$ntree, "OOB"]
})

# Organize results
oob_df <- data.frame(mtry = mtry_grid, OOB_Error = oob)
oob_df

# Select mtry with lowest OOB error
best_mtry <- oob_df$mtry[which.min(oob_df$OOB_Error)]
best_mtry


# ============================================================================
# STEP 2: BUILD THE FINAL RANDOM FOREST MODEL
# ============================================================================

# Train Random Forest using optimal mtry
rf <- randomForest(
  x = dat[, pred_cols],              # Predictors from dat
  y = as.factor(dat$TBMF),           # Response (TBMF as factor)
  ntree = 500,                        # Number of trees
  mtry = best_mtry,                   # Optimal mtry
  importance = TRUE,                  # Calculate variable importance
  keep.forest = TRUE                  # Save forest for predictions
)


# ============================================================================
# STEP 3: EXTERNAL 10-FOLD CROSS-VALIDATION
# ============================================================================

rf_cv <- do.call(rbind, 
  lapply(folds, function(idx_test){  # Use folds from before
    tr <- dat[-idx_test, ]
    te <- dat[idx_test, ]
    
    # Train Random Forest
    m <- randomForest(
      x = tr[, pred_cols],              
      y = as.factor(tr$TBMF),
      ntree = 500,
      mtry = best_mtry,              # Use best_mtry
      importance = TRUE
    )
    
    # Predict on test fold
    p <- predict(m, newdata = te[, pred_cols], type = "prob")[, "1"]
    
    data.frame(ID = te$ID, TBMF = te$TBMF, prob = p)
  })
)

# Calculate performance metrics
rf_cv_metrics <- acc_metrics(rf_cv$TBMF, rf_cv$prob, thr = 0.5)
rf_cv_metrics

# ============================================================================
# STEP 4: VARIABLE IMPORTANCE
# ============================================================================

# Extract importance scores
# HINT: importance() function extracts from rf object
rf_vi <- data.frame(
  Variable = rownames(importance(rf)),
  MeanDecreaseGini = importance(rf)[, "MeanDecreaseGini"],
  MeanDecreaseAccuracy = importance(rf)[, "MeanDecreaseAccuracy"]
)


# Sort by Gini importance
rf_vi <- rf_vi[order(-rf_vi$MeanDecreaseGini), ]  # Sort descending

# Display top 10
head(rf_vi, 10)
```


**Reflection Questions:**

1. Why can OOB error serve as a trustworthy proxy for cross-validation in Random Forests?
*OOB error uses unused data from each bootstrap sample, giving an unbiased estimate of prediction error without separate CV.*

2. How does `mtry` influence the bias–variance trade-off?
*mtry controls tree diversity: low --> high variance, low bias. high to low variance, high bias.*

3. What does the optimal `mtry` value reveal about climate-biome relationship complexity?
*shows how many predictors matter per split. higher values imply more complex climate–biome relationships*

4. For conservation planning, would students recommend the interpretable CART model or the more accurate but "black box" Random Forest?
*in conservation planning, CART would be best for interpretation, but if accuracy is important, Random Forest is better despite being a “black box.”*


---

## Task 5 – Gradient Boosting Machine (18–20 min)

**Objective:**

A Gradient Boosting Machine (GBM) is built—an ensemble method where trees are built sequentially, with each new tree correcting errors from previous trees.

```{r}
# ============================================================================
# STEP 1: BUILD GBM WITH INTERNAL CROSS-VALIDATION
# ============================================================================

# Fit Gradient Boosting Machine
# HINT: distribution="bernoulli" for binary outcome
gbm0 <- gbm(
  formula = ___,                   # Use form from before
  data = ___,                      # Use dat
  distribution = "___",            # "bernoulli" for binary
  n.trees = ___,                   # Max trees to build (5000)
  interaction.depth = ___,         # Tree depth (3)
  shrinkage = ___,                 # Learning rate (0.01)
  n.minobsinnode = ___,            # Min observations per node (10)
  bag.fraction = ___,              # Subsample fraction (0.7)
  cv.folds = ___,                  # CV folds (10)
  keep.data = ___,                 # Keep data? (TRUE)
  verbose = ___                    # Print progress? (FALSE)
)

# Find optimal number of trees
# HINT: gbm.perf() finds best iteration using CV
best_iter <- gbm.perf(___, method = "___", plot.it = ___)

# ============================================================================
# STEP 2: EXTERNAL 10-FOLD CROSS-VALIDATION - THIS CAN TAKE SOME TIME
# ============================================================================

gbm_cv <- do.call(rbind, 
  lapply(___, function(idx_test){  # Use folds
    tr <- dat[___idx_test, ]
    te <- dat[___, ]
    
    # Train GBM
    m <- gbm(
      formula = ___,
      data = ___,
      distribution = "___",
      n.trees = ___,             # Use best_iter
      interaction.depth = ___,
      shrinkage = ___,
      n.minobsinnode = ___,
      bag.fraction = ___,
      keep.data = ___,           # FALSE to save memory
      verbose = ___
    )
    
    # Predict on test fold
    # HINT: predict.gbm() needs n.trees and type="response"
    p <- predict(___, newdata = ___, n.trees = ___, type = "___")
    
    data.frame(ID = ___$ID, TBMF = ___$TBMF, prob = ___)
  })
)

# Calculate performance metrics
gbm_cv_metrics <- acc_metrics(___$___, ___$___, thr = ___)
gbm_cv_metrics

# ============================================================================
# STEP 3: EXTRACT VARIABLE IMPORTANCE
# ============================================================================

# Calculate variable importance
# HINT: summary() on gbm object gives importance
gbm_vi <- summary(___, n.trees = ___, plotit = ___)

# Display top 10
head(___, ___)
```

**Reflection Questions:**

1. How do `shrinkage` and `interaction.depth` affect overfitting risk?
2. Why does `gbm.perf` select fewer trees than the maximum fitted?
3. What does the optimal stopping point reveal about climate-TBMF relationship complexity?

---

## Task 6 – Comparing Variables and Accuracy (8–10 min)

**Objective:**

Two summary tables are created for direct comparison across all three models.

```{r}
# ============================================================================
# STEP 1: CREATE UNIFIED VARIABLE IMPORTANCE TABLE
# ============================================================================

# Extract CART rankings
# HINT: rank() with negative value ranks descending
cart_vars <- data.frame(
  Model = "___",                        # "CART"
  Variable = ___$Variable,              # From cart_vi
  Rank = rank(___$Importance, ties.method = "___")  # Use negative for descending
)

# Extract RF rankings
rf_vars <- data.frame(
  Model = "___",                        # "RF"
  Variable = ___$___,                   # Variable column from rf_vi
  Rank = rank(___$___, ties.method = "___")  # Use MeanDecreaseGini
)

# Extract GBM rankings
gbm_vars <- data.frame(
  Model = "___",                        # "GBM"
  Variable = ___$___,                   # var column from gbm_vi
  Rank = rank(___$___, ties.method = "___")  # Use rel.inf
)

# Combine all tables
# HINT: rbind() stacks data frames vertically
vars_all <- rbind(___, ___, ___)

# Display first 15 rows
head(___, ___)

# ============================================================================
# STEP 2: CREATE UNIFIED PERFORMANCE METRICS TABLE
# ============================================================================

# Combine CV metrics from all models
acc_tab <- rbind(
  data.frame(Model = "___", ___),       # CART (pruned), cart_cv_metrics
  data.frame(Model = "___", ___),       # Random Forest, rf_cv_metrics
  data.frame(Model = "___", ___)        # GBM, gbm_cv_metrics
)

# Display comparison
acc_tab
```

**Reflection Questions:**

1. Why might top-ranked variables differ between models?
2. If two models have equal accuracy, which secondary metric should be prioritized?
3. How might importance scales be harmonized to enable comparison?

---

## Task 7 – Global Prediction (10–12 min)

**Objective:**

The best-performing model is used to predict TBMF probability across the entire world.

```{r}
# ============================================================================
# STEP 1: LOAD GLOBAL CLIMATE GRID
# ============================================================================

# Read worldwide climate data
world <- read.csv("___", stringsAsFactors = ___)[,___]

# Validate data compatibility
# HINT: stopifnot() stops if conditions are FALSE
stopifnot(
  all(___ %in% names(___)),      # All pred_cols in world
  all(c("___","___") %in% names(___))  # x and y coordinates in world
)

# ============================================================================
# STEP 2: AUTOMATICALLY SELECT BEST MODEL
# ============================================================================

# Identify model with highest balanced accuracy
# HINT: which.max() finds position of maximum value
best_name <- ___$Model[which.max(___$___)]  # Find max BalancedAccuracy
best_name

# ============================================================================
# STEP 3: GENERATE GLOBAL PREDICTIONS
# ============================================================================

# Apply winning model
# HINT: grepl() checks if pattern matches text
if (grepl("^CART", ___)) {
  # CART predictions
  world$prob_DBF <- predict(___, newdata = ___, type = "___")[, "___"]
  
} else if (grepl("^Random Forest", ___)) {
  # Random Forest predictions
  world$prob_DBF <- predict(___, newdata = ___[, ___, drop = FALSE], 
                            type = "___")[, "___"]
  
} else {
  # GBM predictions
  world$prob_DBF <- predict(___, newdata = ___, n.trees = ___, 
                            type = "___")
}

# ============================================================================
# STEP 4: CREATE GLOBAL MAP VISUALIZATION
# ============================================================================

# Generate world map
# HINT: ggplot() with aes() sets aesthetics, geom_raster() creates grid
ggplot(___, aes(x = ___, y = ___, fill = ___)) +
  geom_raster(interpolate = ___) +      # FALSE
  coord_fixed() +
  scale_fill_viridis_c(name = "___") +  # "P(TBMF)"
  labs(
    title = paste("Predicted probability of Deciduous Broadleaf Forests –", 
                  ___),                   # best_name
    x = "___",                           # "Longitude"
    y = "___"                            # "Latitude"
  )
```

**Reflection Questions:**

1. What risks arise when predicting outside the training climate envelope (extrapolation)?
2. How should `P(TBMF)` be thresholded to produce a binary presence/absence map?
3. Which uncertainty maps would be valuable additions to this analysis?

---

